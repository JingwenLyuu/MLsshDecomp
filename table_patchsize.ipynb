{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf8c741-35e7-4b42-9005-1c83887e18f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============================================================\n",
      "Evaluating patch size 108×108\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  UBM R²=0.021, BM R²=0.981, U R²=0.862, V R²=0.857\n",
      "\n",
      "============================================================\n",
      "Evaluating patch size 80×80\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  UBM R²=0.010, BM R²=0.969, U R²=0.848, V R²=0.849\n",
      "\n",
      "============================================================\n",
      "Evaluating patch size 54×54\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:255: ZarrUserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7c65cc0bc170>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  UBM R²=-0.196, BM R²=0.931, U R²=0.809, V R²=0.805\n",
      "\n",
      "=== R² Table ===\n",
      "                         UBM                 BM                  U  \\\n",
      "size                                                                 \n",
      "108×108   0.02 (-0.97, 0.69)  0.98 (0.92, 1.00)  0.86 (0.63, 0.98)   \n",
      "80×80     0.01 (-0.94, 0.73)  0.97 (0.87, 1.00)  0.85 (0.57, 0.97)   \n",
      "54×54    -0.20 (-1.76, 0.73)  0.93 (0.71, 1.00)  0.81 (0.45, 0.97)   \n",
      "\n",
      "                         V   Avg  \n",
      "size                              \n",
      "108×108  0.86 (0.54, 0.98)  0.68  \n",
      "80×80    0.85 (0.58, 0.98)  0.67  \n",
      "54×54    0.80 (0.41, 0.97)  0.59  \n",
      "\n",
      "=== Correlation Table ===\n",
      "                        UBM                 BM                  U  \\\n",
      "size                                                                \n",
      "108×108   0.47 (0.06, 0.86)  0.99 (0.97, 1.00)  0.93 (0.81, 0.99)   \n",
      "80×80     0.48 (0.01, 0.87)  0.99 (0.94, 1.00)  0.92 (0.78, 0.99)   \n",
      "54×54    0.50 (-0.05, 0.90)  0.98 (0.90, 1.00)  0.91 (0.72, 0.99)   \n",
      "\n",
      "                         V   Avg  \n",
      "size                              \n",
      "108×108  0.93 (0.78, 0.99)  0.83  \n",
      "80×80    0.92 (0.77, 0.99)  0.83  \n",
      "54×54    0.91 (0.70, 0.99)  0.82  \n",
      "\n",
      "============================================================\n",
      "LATEX OUTPUT\n",
      "============================================================\n",
      "\\begin{table}\n",
      "\\settablenum{S3}\n",
      "\\caption{R$^2$ Performance Comparison of ZCA+SST Model at Different Mesh Resolutions for UBM, BM, and Geostrophic Velocity Prediction. 5th and 95th percentiles are shown in parentheses.}\n",
      "\\centering\n",
      "\\begin{tabular}{l c c c c c}\n",
      "\\hline\n",
      "\\textbf{Mesh Size} & \\textbf{UBM (5th, 95th)} & \\textbf{BM (5th, 95th)} & \\textbf{U (5th, 95th)} & \\textbf{V (5th, 95th)} & \\textbf{Avg. R$^2$} \\\\\n",
      "\\hline\n",
      "108×108 & 0.02 (-0.97, 0.69) & 0.98 (0.92, 1.00) & 0.86 (0.63, 0.98) & 0.86 (0.54, 0.98) & 0.68 \\\\\n",
      "80×80 & 0.01 (-0.94, 0.73) & 0.97 (0.87, 1.00) & 0.85 (0.57, 0.97) & 0.85 (0.58, 0.98) & 0.67 \\\\\n",
      "54×54 & -0.20 (-1.76, 0.73) & 0.93 (0.71, 1.00) & 0.81 (0.45, 0.97) & 0.80 (0.41, 0.97) & 0.59 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\settablenum{S4}\n",
      "\\caption{Correlation Performance Comparison of ZCA+SST Model at Different Mesh Resolutions for UBM, BM, and Geostrophic Velocity Prediction. 5th and 95th percentiles are shown in parentheses.}\n",
      "\\centering\n",
      "\\begin{tabular}{l c c c c c}\n",
      "\\hline\n",
      "\\textbf{Mesh Size} & \\textbf{UBM (5th, 95th)} & \\textbf{BM (5th, 95th)} & \\textbf{U (5th, 95th)} & \\textbf{V (5th, 95th)} & \\textbf{Avg. corr} \\\\\n",
      "\\hline\n",
      "108×108 & 0.47 (0.06, 0.86) & 0.99 (0.97, 1.00) & 0.93 (0.81, 0.99) & 0.93 (0.78, 0.99) & 0.83 \\\\\n",
      "80×80 & 0.48 (0.01, 0.87) & 0.99 (0.94, 1.00) & 0.92 (0.78, 0.99) & 0.92 (0.77, 0.99) & 0.83 \\\\\n",
      "54×54 & 0.50 (-0.05, 0.90) & 0.98 (0.90, 1.00) & 0.91 (0.72, 0.99) & 0.91 (0.70, 0.99) & 0.82 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from train import apply_inverse_zca_whitening_4d_torch\n",
    "from unet import UNet\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "setup_random_seeds(42)\n",
    "device = get_device()\n",
    "\n",
    "base_path = \"gs://leap-persistent/YueWang/SSH/data\"\n",
    "storage_opts = {\"token\": \"cloud\", \"asynchronous\": False}\n",
    "\n",
    "g = 9.81\n",
    "dx_map = {54: 1500.0, 80: 1500.0, 108: 1500.0}  # adjust if grid spacing differs\n",
    "dy_map = {54: 1500.0, 80: 1500.0, 108: 1500.0}\n",
    "f_cor = -8.6e-5\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG: patch sizes and checkpoint paths\n",
    "# ============================================================================\n",
    "configs = [\n",
    "    {\"size\": 108, \"checkpoint\": \"/home/jovyan/GRL_ssh/checkpoints/ps108.pth\",\n",
    "     \"train\": \"train_108_sst.zarr\", \"test\": \"test_108_sst.zarr\", \"zca\": \"zca_108.zarr\"},\n",
    "    {\"size\": 80, \"checkpoint\": \"/home/jovyan/GRL_ssh/checkpoints/sst_ssh.pth\",\n",
    "     \"train\": \"train_80_sst.zarr\", \"test\": \"test_80_sst.zarr\", \"zca\": \"zca_80.zarr\"},\n",
    "    {\"size\": 54, \"checkpoint\": \"/home/jovyan/GRL_ssh/checkpoints/ps54.pth\",\n",
    "     \"train\": \"train_54_sst.zarr\", \"test\": \"test_54_sst.zarr\", \"zca\": \"zca_54.zarr\"},\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "def geostrophic_vel(field_2d, dx, dy):\n",
    "    dη_dy = np.gradient(field_2d, dy, axis=0, edge_order=2)\n",
    "    dη_dx = np.gradient(field_2d, dx, axis=1, edge_order=2)\n",
    "    u = -g / f_cor * dη_dy\n",
    "    v =  g / f_cor * dη_dx\n",
    "    return u, v\n",
    "\n",
    "def r2_corr_arrays(truth, pred):\n",
    "    \"\"\"Per-sample R² and correlation with mean, p05, p95.\"\"\"\n",
    "    r2_vals, c_vals = [], []\n",
    "    for t, p in zip(truth, pred):\n",
    "        m = np.isfinite(t) & np.isfinite(p)\n",
    "        if m.sum() < 2:\n",
    "            r2_vals.append(np.nan); c_vals.append(np.nan)\n",
    "        else:\n",
    "            r2_vals.append(r2_score(t[m], p[m]))\n",
    "            c_vals.append(np.corrcoef(t[m], p[m])[0, 1])\n",
    "    r2_vals, c_vals = np.array(r2_vals), np.array(c_vals)\n",
    "    return (\n",
    "        (np.nanmean(r2_vals), np.nanpercentile(r2_vals, 5), np.nanpercentile(r2_vals, 95)),\n",
    "        (np.nanmean(c_vals), np.nanpercentile(c_vals, 5), np.nanpercentile(c_vals, 95))\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE EACH PATCH SIZE\n",
    "# ============================================================================\n",
    "all_records_r2 = []\n",
    "all_records_corr = []\n",
    "\n",
    "for cfg in configs:\n",
    "    ps = cfg[\"size\"]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating patch size {ps}×{ps}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load data\n",
    "    train_ds = open_zarr(f\"{base_path}/{cfg['train']}\", storage_opts)\n",
    "    test_ds = open_zarr(f\"{base_path}/{cfg['test']}\", storage_opts)\n",
    "    zca_ds = open_zarr(f\"{base_path}/{cfg['zca']}\", storage_opts)\n",
    "    \n",
    "    Vt = torch.from_numpy(zca_ds.ubm_Vt.values).float().to(device)\n",
    "    scale = torch.from_numpy(zca_ds.ubm_scale.values).float().to(device)\n",
    "    mean = torch.from_numpy(zca_ds.ubm_mean.values).float().to(device)\n",
    "    \n",
    "    # Normalization from training set\n",
    "    x_train_ssh = torch.from_numpy(train_ds.ssh.values).float().unsqueeze(1).to(device)\n",
    "    x_train_sst = torch.from_numpy(train_ds.sst.values).float().unsqueeze(1).to(device)\n",
    "    x_train = torch.cat([x_train_ssh, x_train_sst], dim=1)\n",
    "    _, min_vals, max_vals = min_max_normalize(x_train)\n",
    "    del x_train, x_train_ssh, x_train_sst  # free memory\n",
    "    \n",
    "    # Test data\n",
    "    x_test_ssh = torch.from_numpy(test_ds.ssh.values).float().unsqueeze(1).to(device)\n",
    "    x_test_sst = torch.from_numpy(test_ds.sst.values).float().unsqueeze(1).to(device)\n",
    "    x_test = torch.cat([x_test_ssh, x_test_sst], dim=1)\n",
    "    x_test_norm, _, _ = min_max_normalize(x_test, min_vals, max_vals)\n",
    "    \n",
    "    y_test_phys = torch.from_numpy(test_ds.ubm.values).float().unsqueeze(1).to(device)\n",
    "    y_test_zca = torch.from_numpy(test_ds.zca_ubm.values).float().unsqueeze(1).to(device)\n",
    "    y_test = torch.cat([y_test_phys, y_test_zca], dim=1)\n",
    "    \n",
    "    test_loader = DataLoader(TensorDataset(x_test_norm, y_test), batch_size=128, shuffle=False)\n",
    "    \n",
    "    # Load model\n",
    "    model = UNet(in_channels=2, out_channels=2, initial_features=32, depth=4).to(device)\n",
    "    ckpt = torch.load(cfg[\"checkpoint\"], map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Run inference\n",
    "    ubm_preds, ubm_trues, ssh_originals = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (bx, by) in enumerate(test_loader):\n",
    "            bs = i * test_loader.batch_size\n",
    "            be = min(bs + test_loader.batch_size, len(test_loader.dataset))\n",
    "            idx = list(range(bs, be))\n",
    "            \n",
    "            bx = bx.to(device)\n",
    "            outputs = model(bx)\n",
    "            \n",
    "            mu_zca = outputs[:, 0:1, ...]\n",
    "            ubm_pred = apply_inverse_zca_whitening_4d_torch(mu_zca, Vt, scale, mean)\n",
    "            \n",
    "            ubm_preds.append(ubm_pred.squeeze(1).cpu().numpy())\n",
    "            ubm_trues.append(by[:, 0, ...].cpu().numpy())\n",
    "            ssh_originals.append(x_test_ssh[idx].squeeze(1).cpu().numpy())\n",
    "    \n",
    "    ubm_pred_all = np.concatenate(ubm_preds, axis=0)\n",
    "    ubm_true_all = np.concatenate(ubm_trues, axis=0)\n",
    "    ssh_all = np.concatenate(ssh_originals, axis=0)\n",
    "    \n",
    "    bm_pred_all = ssh_all - ubm_pred_all\n",
    "    bm_true_all = ssh_all - ubm_true_all\n",
    "    \n",
    "    # Clean mask\n",
    "    clean = ~np.isnan(ubm_true_all).reshape(ubm_true_all.shape[0], -1).any(axis=1)\n",
    "    \n",
    "    # Flatten clean samples\n",
    "    def fc(arr):\n",
    "        return arr[clean].reshape(clean.sum(), -1)\n",
    "    \n",
    "    # Metrics\n",
    "    dx_val, dy_val = dx_map[ps], dy_map[ps]\n",
    "    \n",
    "    (ubm_r2, ubm_r2_5, ubm_r2_95), (ubm_c, ubm_c_5, ubm_c_95) = r2_corr_arrays(fc(ubm_true_all), fc(ubm_pred_all))\n",
    "    (bm_r2, bm_r2_5, bm_r2_95), (bm_c, bm_c_5, bm_c_95) = r2_corr_arrays(fc(bm_true_all), fc(bm_pred_all))\n",
    "    \n",
    "    # Geostrophic velocities\n",
    "    u_true = np.array([geostrophic_vel(f, dx_val, dy_val)[0] for f in bm_true_all])\n",
    "    v_true = np.array([geostrophic_vel(f, dx_val, dy_val)[1] for f in bm_true_all])\n",
    "    u_pred = np.array([geostrophic_vel(f, dx_val, dy_val)[0] for f in bm_pred_all])\n",
    "    v_pred = np.array([geostrophic_vel(f, dx_val, dy_val)[1] for f in bm_pred_all])\n",
    "    \n",
    "    (u_r2, u_r2_5, u_r2_95), (u_c, u_c_5, u_c_95) = r2_corr_arrays(fc(u_true), fc(u_pred))\n",
    "    (v_r2, v_r2_5, v_r2_95), (v_c, v_c_5, v_c_95) = r2_corr_arrays(fc(v_true), fc(v_pred))\n",
    "    \n",
    "    avg_r2 = np.nanmean([ubm_r2, bm_r2, u_r2, v_r2])\n",
    "    avg_c = np.nanmean([ubm_c, bm_c, u_c, v_c])\n",
    "    \n",
    "    all_records_r2.append({\n",
    "        \"size\": f\"{ps}×{ps}\",\n",
    "        \"UBM\": f\"{ubm_r2:.2f} ({ubm_r2_5:.2f}, {ubm_r2_95:.2f})\",\n",
    "        \"BM\": f\"{bm_r2:.2f} ({bm_r2_5:.2f}, {bm_r2_95:.2f})\",\n",
    "        \"U\": f\"{u_r2:.2f} ({u_r2_5:.2f}, {u_r2_95:.2f})\",\n",
    "        \"V\": f\"{v_r2:.2f} ({v_r2_5:.2f}, {v_r2_95:.2f})\",\n",
    "        \"Avg\": f\"{avg_r2:.2f}\"\n",
    "    })\n",
    "    \n",
    "    all_records_corr.append({\n",
    "        \"size\": f\"{ps}×{ps}\",\n",
    "        \"UBM\": f\"{ubm_c:.2f} ({ubm_c_5:.2f}, {ubm_c_95:.2f})\",\n",
    "        \"BM\": f\"{bm_c:.2f} ({bm_c_5:.2f}, {bm_c_95:.2f})\",\n",
    "        \"U\": f\"{u_c:.2f} ({u_c_5:.2f}, {u_c_95:.2f})\",\n",
    "        \"V\": f\"{v_c:.2f} ({v_c_5:.2f}, {v_c_95:.2f})\",\n",
    "        \"Avg\": f\"{avg_c:.2f}\"\n",
    "    })\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, x_test_ssh, x_test_sst, x_test, x_test_norm, y_test\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"  UBM R²={ubm_r2:.3f}, BM R²={bm_r2:.3f}, U R²={u_r2:.3f}, V R²={v_r2:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PRINT RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n=== R² Table ===\")\n",
    "df_r2 = pd.DataFrame(all_records_r2).set_index(\"size\")\n",
    "print(df_r2)\n",
    "\n",
    "print(\"\\n=== Correlation Table ===\")\n",
    "df_corr = pd.DataFrame(all_records_corr).set_index(\"size\")\n",
    "print(df_corr)\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE LATEX\n",
    "# ============================================================================\n",
    "def to_latex_table(df, caption, table_num, metric_name):\n",
    "    lines = []\n",
    "    lines.append(r\"\\begin{table}\")\n",
    "    lines.append(f\"\\\\settablenum{{{table_num}}}\")\n",
    "    lines.append(f\"\\\\caption{{{caption}}}\")\n",
    "    lines.append(r\"\\centering\")\n",
    "    lines.append(r\"\\begin{tabular}{l c c c c c}\")\n",
    "    lines.append(r\"\\hline\")\n",
    "    lines.append(f\"\\\\textbf{{Mesh Size}} & \\\\textbf{{UBM (5th, 95th)}} & \\\\textbf{{BM (5th, 95th)}} & \\\\textbf{{U (5th, 95th)}} & \\\\textbf{{V (5th, 95th)}} & \\\\textbf{{Avg. {metric_name}}} \\\\\\\\\")\n",
    "    lines.append(r\"\\hline\")\n",
    "    for size, row in df.iterrows():\n",
    "        lines.append(f\"{size} & {row['UBM']} & {row['BM']} & {row['U']} & {row['V']} & {row['Avg']} \\\\\\\\\")\n",
    "    lines.append(r\"\\hline\")\n",
    "    lines.append(r\"\\end{tabular}\")\n",
    "    lines.append(r\"\\end{table}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LATEX OUTPUT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(to_latex_table(df_r2, \n",
    "    r\"R$^2$ Performance Comparison of ZCA+SST Model at Different Mesh Resolutions for UBM, BM, and Geostrophic Velocity Prediction. 5th and 95th percentiles are shown in parentheses.\",\n",
    "    \"S3\", r\"R$^2$\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(to_latex_table(df_corr,\n",
    "    r\"Correlation Performance Comparison of ZCA+SST Model at Different Mesh Resolutions for UBM, BM, and Geostrophic Velocity Prediction. 5th and 95th percentiles are shown in parentheses.\",\n",
    "    \"S4\", \"corr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5017d-8662-460d-80cd-25468da55b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
