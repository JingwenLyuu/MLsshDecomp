{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73b9994-d2e1-41fe-aaee-07d7f139cfcf",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3da2a7-6377-478f-99d4-45d86852f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:256: UserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7e1f6f6f0950>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:256: UserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7e1f6f6f0950>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/storage/_fsspec.py:256: UserWarning: fs (<gcsfs.core.GCSFileSystem object at 0x7e1f6f6f0950>) was not created with `asynchronous=True`, this may lead to surprising behavior\n",
      "  return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n",
      "/tmp/ipykernel_105/2914359427.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/home/jovyan/GRL_ssh/checkpoints/sst_ssh.pth', map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from train import apply_inverse_zca_whitening_4d_torch\n",
    "from unet import UNet  \n",
    "from utils import *\n",
    "\n",
    "setup_random_seeds(42)\n",
    "device = get_device()\n",
    "\n",
    "base_path = \"gs://leap-persistent/YueWang/SSH/data\"\n",
    "storage_opts = {\"token\": \"cloud\", \"asynchronous\": False}\n",
    "\n",
    "train = open_zarr(f\"{base_path}/train_80_sst.zarr\", storage_opts)\n",
    "test = open_zarr(f\"{base_path}/test_80_sst.zarr\", storage_opts)\n",
    "zca = open_zarr(f\"{base_path}/zca_80.zarr\", storage_opts)\n",
    "\n",
    "Vt = torch.from_numpy(zca.ubm_Vt.values).float().to(device)\n",
    "scale = torch.from_numpy(zca.ubm_scale.values).float().to(device)\n",
    "mean = torch.from_numpy(zca.ubm_mean.values).float().to(device)\n",
    "\n",
    "# Model 1: ZCA NLL Loss with SSH+SST input\n",
    "\n",
    "# Prepare training data for normalization statistics (SSH+SST)\n",
    "x_train_ssh = torch.from_numpy(train.ssh.values).float().unsqueeze(1).to(device)\n",
    "x_train_sst = torch.from_numpy(train.sst.values).float().unsqueeze(1).to(device)\n",
    "x_train = torch.cat([x_train_ssh, x_train_sst], dim=1)\n",
    "x_train_normalized, min_vals_sst, max_vals_sst = min_max_normalize(x_train)\n",
    "\n",
    "# Prepare test data (SSH+SST) \n",
    "x_test_ssh_original = torch.from_numpy(test.ssh.values).float().unsqueeze(1).to(device)\n",
    "x_test_sst_original = torch.from_numpy(test.sst.values).float().unsqueeze(1).to(device)\n",
    "x_test_original = torch.cat([x_test_ssh_original, x_test_sst_original], dim=1)\n",
    "\n",
    "# Normalize test data for model input\n",
    "x_test_normalized, _, _ = min_max_normalize(x_test_original, min_vals_sst, max_vals_sst)\n",
    "\n",
    "# Prepare test targets\n",
    "y_test_physical = torch.from_numpy(test.ubm.values).float().unsqueeze(1).to(device)\n",
    "y_test_zca = torch.from_numpy(test.zca_ubm.values).float().unsqueeze(1).to(device)\n",
    "y_test = torch.cat([y_test_physical, y_test_zca], dim=1)\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset_sst = TensorDataset(x_test_normalized, y_test)\n",
    "test_loader_sst = DataLoader(test_dataset_sst, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model_sst_ssh = UNet(in_channels=2, out_channels=2, initial_features=32, depth=4)\n",
    "model_sst_ssh.to(device)\n",
    "\n",
    "checkpoint = torch.load('/home/jovyan/GRL_ssh/checkpoints/sst_ssh.pth', map_location=device)\n",
    "model_sst_ssh.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate model\n",
    "model_sst_ssh.eval()\n",
    "results_sst_ssh = {\n",
    "    'ssh': [], 'sst': [], 'ubm_true': [], 'bm_true': [],\n",
    "    'ubm_pred_mu': [], 'bm_pred_mu': [],\n",
    "    'ubm_pred_ensembles': [], 'bm_pred_ensembles': []\n",
    "}\n",
    "\n",
    "sample_indices = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y) in enumerate(test_loader_sst):\n",
    "        \n",
    "        batch_start = i * test_loader_sst.batch_size\n",
    "        batch_end = min(batch_start + test_loader_sst.batch_size, len(test_dataset_sst))\n",
    "        current_batch_indices = list(range(batch_start, batch_end))\n",
    "        sample_indices.extend(current_batch_indices)\n",
    "        \n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y_physical = batch_y[:, 0:1, ...].to(device)\n",
    "\n",
    "        outputs = model_sst_ssh(batch_x)\n",
    "        \n",
    "        ssh_batch_original = x_test_ssh_original[current_batch_indices]\n",
    "        sst_batch_original = x_test_sst_original[current_batch_indices]\n",
    "        \n",
    "        ubm_true = batch_y_physical\n",
    "        bm_true = ssh_batch_original - ubm_true\n",
    "        \n",
    "        # Predicted mean in physical space\n",
    "        mu_zca = outputs[:, 0, ...]\n",
    "        log_sigma_zca = outputs[:, 1, ...]\n",
    "        mu_zca_expanded = mu_zca.unsqueeze(1)\n",
    "        ubm_pred_mu = apply_inverse_zca_whitening_4d_torch(mu_zca_expanded, Vt, scale, mean)\n",
    "        \n",
    "        bm_pred_mu = ssh_batch_original - ubm_pred_mu\n",
    "        \n",
    "        # Generate ensemble samples\n",
    "        zca_samples = generate_gaussian_samples(mu_zca, log_sigma_zca, n_samples=30)\n",
    "        B, n_samples, H, W = zca_samples.shape\n",
    "        zca_samples_flat = zca_samples.reshape(B * n_samples, 1, H, W)\n",
    "        ubm_samples_flat = apply_inverse_zca_whitening_4d_torch(zca_samples_flat, Vt, scale, mean)\n",
    "        ubm_samples = ubm_samples_flat.reshape(B, n_samples, 1, H, W)\n",
    "        \n",
    "        # Use original SSH for ensemble BM calculation\n",
    "        ssh_expanded = ssh_batch_original.unsqueeze(1).expand(-1, n_samples, -1, -1, -1)\n",
    "        bm_samples = ssh_expanded - ubm_samples\n",
    "        \n",
    "        # Store results \n",
    "        results_sst_ssh['ssh'].append(ssh_batch_original.cpu().numpy())\n",
    "        results_sst_ssh['sst'].append(sst_batch_original.cpu().numpy())\n",
    "        results_sst_ssh['ubm_true'].append(ubm_true.cpu().numpy())\n",
    "        results_sst_ssh['bm_true'].append(bm_true.cpu().numpy())\n",
    "        results_sst_ssh['ubm_pred_mu'].append(ubm_pred_mu.cpu().numpy())\n",
    "        results_sst_ssh['bm_pred_mu'].append(bm_pred_mu.cpu().numpy())\n",
    "        results_sst_ssh['ubm_pred_ensembles'].append(ubm_samples.cpu().numpy())\n",
    "        results_sst_ssh['bm_pred_ensembles'].append(bm_samples.cpu().numpy())\n",
    "\n",
    "for key in results_sst_ssh:\n",
    "    results_sst_ssh[key] = np.concatenate(results_sst_ssh[key], axis=0)\n",
    "\n",
    "print(\"Model 1 evaluation complete!\")\n",
    "\n",
    "# Model 2: SSH input only\n",
    "\n",
    "# Prepare training data for normalization statistics (SSH only)\n",
    "x_train_ssh_only = torch.from_numpy(train.ssh.values).float().unsqueeze(1).to(device)\n",
    "x_train_normalized_ssh, min_vals_ssh, max_vals_ssh = min_max_normalize(x_train_ssh_only)\n",
    "\n",
    "# Prepare test data (SSH only)\n",
    "x_test_ssh_only_original = torch.from_numpy(test.ssh.values).float().unsqueeze(1).to(device)\n",
    "x_test_normalized_ssh, _, _ = min_max_normalize(x_test_ssh_only_original, min_vals_ssh, max_vals_ssh)\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset_ssh = TensorDataset(x_test_normalized_ssh, y_test)\n",
    "test_loader_ssh = DataLoader(test_dataset_ssh, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model_ssh_only = UNet(in_channels=1, out_channels=2, initial_features=32, depth=4)\n",
    "model_ssh_only.to(device)\n",
    "\n",
    "checkpoint = torch.load('/home/jovyan/GRL_ssh/checkpoints/ssh_input_only.pth', map_location=device)\n",
    "model_ssh_only.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate model\n",
    "model_ssh_only.eval()\n",
    "results_ssh_only = {\n",
    "    'ssh': [], 'ubm_true': [], 'bm_true': [],\n",
    "    'ubm_pred_mu': [], 'bm_pred_mu': [],\n",
    "    'ubm_pred_ensembles': [], 'bm_pred_ensembles': []\n",
    "}\n",
    "\n",
    "sample_indices_ssh = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y) in enumerate(test_loader_ssh):\n",
    "        \n",
    "        batch_start = i * test_loader_ssh.batch_size\n",
    "        batch_end = min(batch_start + test_loader_ssh.batch_size, len(test_dataset_ssh))\n",
    "        current_batch_indices = list(range(batch_start, batch_end))\n",
    "        sample_indices_ssh.extend(current_batch_indices)\n",
    "            \n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y_physical = batch_y[:, 0:1, ...].to(device)\n",
    "        \n",
    "        outputs = model_ssh_only(batch_x)\n",
    "        \n",
    "        ssh_batch_original = x_test_ssh_only_original[current_batch_indices]\n",
    "        \n",
    "        ubm_true = batch_y_physical\n",
    "        bm_true = ssh_batch_original - ubm_true\n",
    "        \n",
    "        # Predicted mean in physical space\n",
    "        mu_zca = outputs[:, 0, ...]\n",
    "        log_sigma_zca = outputs[:, 1, ...]\n",
    "        mu_zca_expanded = mu_zca.unsqueeze(1)\n",
    "        ubm_pred_mu = apply_inverse_zca_whitening_4d_torch(mu_zca_expanded, Vt, scale, mean)\n",
    "        # BM prediction using original SSH scale\n",
    "        bm_pred_mu = ssh_batch_original - ubm_pred_mu\n",
    "        \n",
    "        # Generate ensemble samples\n",
    "        zca_samples = generate_gaussian_samples(mu_zca, log_sigma_zca, n_samples=30)\n",
    "        B, n_samples, H, W = zca_samples.shape\n",
    "        zca_samples_flat = zca_samples.reshape(B * n_samples, 1, H, W)\n",
    "        ubm_samples_flat = apply_inverse_zca_whitening_4d_torch(zca_samples_flat, Vt, scale, mean)\n",
    "        ubm_samples = ubm_samples_flat.reshape(B, n_samples, 1, H, W)\n",
    "        \n",
    "        # Use original SSH for ensemble BM calculation\n",
    "        ssh_expanded = ssh_batch_original.unsqueeze(1).expand(-1, n_samples, -1, -1, -1)\n",
    "        bm_samples = ssh_expanded - ubm_samples\n",
    "        \n",
    "        # Store results \n",
    "        results_ssh_only['ssh'].append(ssh_batch_original.cpu().numpy())\n",
    "        results_ssh_only['ubm_true'].append(ubm_true.cpu().numpy())\n",
    "        results_ssh_only['bm_true'].append(bm_true.cpu().numpy())\n",
    "        results_ssh_only['ubm_pred_mu'].append(ubm_pred_mu.cpu().numpy())\n",
    "        results_ssh_only['bm_pred_mu'].append(bm_pred_mu.cpu().numpy())\n",
    "        results_ssh_only['ubm_pred_ensembles'].append(ubm_samples.cpu().numpy())\n",
    "        results_ssh_only['bm_pred_ensembles'].append(bm_samples.cpu().numpy())\n",
    "\n",
    "for key in results_ssh_only:\n",
    "    results_ssh_only[key] = np.concatenate(results_ssh_only[key], axis=0)\n",
    "\n",
    "print(\"Model 2 evaluation complete!\")\n",
    "\n",
    "# Model 3: MSE loss only\n",
    "model_mse_only = UNet(in_channels=2, out_channels=2, initial_features=32, depth=4)\n",
    "model_mse_only.to(device)\n",
    "\n",
    "checkpoint = torch.load('/home/jovyan/GRL_ssh/checkpoints/mse_loss_only.pth', map_location=device)\n",
    "model_mse_only.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate model\n",
    "model_mse_only.eval()\n",
    "results_mse_only = {\n",
    "    'ssh': [], 'sst': [], 'ubm_true': [], 'bm_true': [],\n",
    "    'ubm_pred_mu': [], 'bm_pred_mu': []\n",
    "}\n",
    "\n",
    "sample_indices_mse = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y) in enumerate(test_loader_sst):  \n",
    "        \n",
    "        batch_start = i * test_loader_sst.batch_size\n",
    "        batch_end = min(batch_start + test_loader_sst.batch_size, len(test_dataset_sst))\n",
    "        current_batch_indices = list(range(batch_start, batch_end))\n",
    "        sample_indices_mse.extend(current_batch_indices)\n",
    "            \n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y_physical = batch_y[:, 0:1, ...].to(device)\n",
    "        \n",
    "        outputs = model_mse_only(batch_x)\n",
    "        \n",
    "        # Use original scale SSH and SST for BM calculation\n",
    "        ssh_batch_original = x_test_ssh_original[current_batch_indices]\n",
    "        sst_batch_original = x_test_sst_original[current_batch_indices]\n",
    "        \n",
    "        ubm_true = batch_y_physical\n",
    "        bm_true = ssh_batch_original - ubm_true\n",
    "        \n",
    "        # For MSE model, only use mean prediction (no sampling)\n",
    "        mu_zca_expanded = outputs[:, 0:1, ...]  # Use first channel as mean\n",
    "        ubm_pred_mu = apply_inverse_zca_whitening_4d_torch(mu_zca_expanded, Vt, scale, mean)\n",
    "        # BM prediction using original SSH scale\n",
    "        bm_pred_mu = ssh_batch_original - ubm_pred_mu\n",
    "        \n",
    "        # Store results\n",
    "        results_mse_only['ssh'].append(ssh_batch_original.cpu().numpy())\n",
    "        results_mse_only['sst'].append(sst_batch_original.cpu().numpy())\n",
    "        results_mse_only['ubm_true'].append(ubm_true.cpu().numpy())\n",
    "        results_mse_only['bm_true'].append(bm_true.cpu().numpy())\n",
    "        results_mse_only['ubm_pred_mu'].append(ubm_pred_mu.cpu().numpy())\n",
    "        results_mse_only['bm_pred_mu'].append(bm_pred_mu.cpu().numpy())\n",
    "\n",
    "for key in results_mse_only:\n",
    "    results_mse_only[key] = np.concatenate(results_mse_only[key], axis=0)\n",
    "\n",
    "print(\"Model 3 evaluation complete!\")\n",
    "\n",
    "# Create xarray datasets and save results\n",
    "models_results = [\n",
    "    ('sst_ssh', results_sst_ssh, True, True),\n",
    "    ('ssh_only', results_ssh_only, True, False), \n",
    "    ('mse_only', results_mse_only, False, True)\n",
    "]\n",
    "\n",
    "# Store all datasets\n",
    "eval_datasets = {}\n",
    "\n",
    "for model_name, results, has_ensembles, has_sst in models_results:\n",
    "    print(f\"Creating dataset for {model_name}...\")\n",
    "    \n",
    "    eval_dataset = create_evaluation_dataset(results, model_name, has_ensembles, has_sst)\n",
    "    \n",
    "    # Store the dataset\n",
    "    eval_datasets[model_name] = eval_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2740f89-6f35-4abe-96d2-d4cd10c3f365",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea6640c-6c73-4560-9cd4-ad021e392fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples=3645, clean=2907, skipped=738\n",
      "Processing model: sst_ssh\n",
      "Processing model: ssh_only\n",
      "Processing model: mse_only\n",
      "\n",
      "=== Summary metrics (mean over clean samples) ===\n",
      "          UBM_R2  UBM_corr  BM_R2  BM_corr   u_R2  u_corr   v_R2  v_corr  \\\n",
      "model                                                                      \n",
      "sst_ssh    0.010     0.478  0.969    0.987  0.848   0.925  0.849   0.924   \n",
      "ssh_only  -0.176     0.401  0.955    0.980  0.812   0.904  0.813   0.904   \n",
      "mse_only  -0.254     0.389  0.958    0.981  0.761   0.885  0.754   0.883   \n",
      "\n",
      "          avg_all  \n",
      "model              \n",
      "sst_ssh     0.749  \n",
      "ssh_only    0.699  \n",
      "mse_only    0.670  \n",
      "\n",
      "=== Summary metrics (5th percentile over clean samples) ===\n",
      "          UBM_R2  UBM_corr  BM_R2  BM_corr   u_R2  u_corr   v_R2  v_corr  \\\n",
      "model                                                                      \n",
      "sst_ssh   -0.938     0.010  0.869    0.942  0.567   0.778  0.582   0.774   \n",
      "ssh_only  -1.349    -0.107  0.800    0.909  0.445   0.701  0.399   0.682   \n",
      "mse_only  -1.579    -0.077  0.813    0.915  0.277   0.648  0.206   0.623   \n",
      "\n",
      "          avg_all  \n",
      "model              \n",
      "sst_ssh     0.448  \n",
      "ssh_only    0.310  \n",
      "mse_only    0.228  \n",
      "\n",
      "=== Summary metrics (95th percentile over clean samples) ===\n",
      "          UBM_R2  UBM_corr  BM_R2  BM_corr   u_R2  u_corr   v_R2  v_corr  \\\n",
      "model                                                                      \n",
      "sst_ssh    0.726     0.874  0.999    1.000  0.975   0.988  0.977   0.989   \n",
      "ssh_only   0.590     0.816  0.999    1.000  0.973   0.987  0.976   0.988   \n",
      "mse_only   0.613     0.816  0.999    0.999  0.967   0.984  0.968   0.984   \n",
      "\n",
      "          avg_all  \n",
      "model              \n",
      "sst_ssh     0.941  \n",
      "ssh_only    0.916  \n",
      "mse_only    0.916  \n",
      "\n",
      "=== Combined Summary (Mean ± Range) ===\n",
      "                          UBM_R2               UBM_corr                 BM_R2  \\\n",
      "model                                                                           \n",
      "sst_ssh     0.01 (-0.938, 0.726)    0.478 (0.01, 0.874)  0.969 (0.869, 0.999)   \n",
      "ssh_only   -0.176 (-1.349, 0.59)  0.401 (-0.107, 0.816)    0.955 (0.8, 0.999)   \n",
      "mse_only  -0.254 (-1.579, 0.613)  0.389 (-0.077, 0.816)  0.958 (0.813, 0.999)   \n",
      "\n",
      "                       BM_corr                  u_R2                u_corr  \\\n",
      "model                                                                        \n",
      "sst_ssh     0.987 (0.942, 1.0)  0.848 (0.567, 0.975)  0.925 (0.778, 0.988)   \n",
      "ssh_only     0.98 (0.909, 1.0)  0.812 (0.445, 0.973)  0.904 (0.701, 0.987)   \n",
      "mse_only  0.981 (0.915, 0.999)  0.761 (0.277, 0.967)  0.885 (0.648, 0.984)   \n",
      "\n",
      "                          v_R2                v_corr               avg_all  \n",
      "model                                                                       \n",
      "sst_ssh   0.849 (0.582, 0.977)  0.924 (0.774, 0.989)  0.749 (0.448, 0.941)  \n",
      "ssh_only  0.813 (0.399, 0.976)  0.904 (0.682, 0.988)   0.699 (0.31, 0.916)  \n",
      "mse_only  0.754 (0.206, 0.968)  0.883 (0.623, 0.984)   0.67 (0.228, 0.916)  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# -------------------------------------------------\n",
    "# constants & grid info\n",
    "# -------------------------------------------------\n",
    "g      = 9.81       # m s⁻²\n",
    "dx     = 1_500.0    # m   (1.5 km grid)\n",
    "dy     = 1_500.0    # m\n",
    "f_cor  = -8.6e-5    # s⁻¹ (Agulhas region)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# clean-sample mask (no NaNs in truth UBM)\n",
    "# -------------------------------------------------\n",
    "clean_mask = ~test.ubm.isnull().any(dim=(\"i\", \"j\")).values\n",
    "clean_idx  = np.where(clean_mask)[0]\n",
    "print(f\"Total samples={len(clean_mask)}, clean={len(clean_idx)}, skipped={len(clean_mask)-len(clean_idx)}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# helpers\n",
    "# -------------------------------------------------\n",
    "def flatten_clean(da):\n",
    "    \"\"\"Return (Nclean, P) NumPy array of flattened spatial dims.\"\"\"\n",
    "    return da.isel(sample=clean_idx).stack(pixels=(\"i\", \"j\")).values\n",
    "\n",
    "def r2_corr(truth, pred):\n",
    "    \"\"\"Per-sample R² & corr, with mean, 5th and 95th percentiles over clean samples.\"\"\"\n",
    "    r2_vals, c_vals = [], []\n",
    "    for t, p in zip(truth, pred):\n",
    "        m = np.isfinite(t) & np.isfinite(p)\n",
    "        if m.sum() < 2:\n",
    "            r2_vals.append(np.nan)\n",
    "            c_vals.append(np.nan)\n",
    "        else:\n",
    "            r2_vals.append(r2_score(t[m], p[m]))\n",
    "            c_vals.append(np.corrcoef(t[m], p[m])[0, 1])\n",
    "    \n",
    "    # Calculate mean and percentiles\n",
    "    r2_mean = np.nanmean(r2_vals)\n",
    "    r2_p05 = np.nanpercentile(r2_vals, 5)\n",
    "    r2_p95 = np.nanpercentile(r2_vals, 95)\n",
    "    \n",
    "    c_mean = np.nanmean(c_vals)\n",
    "    c_p05 = np.nanpercentile(c_vals, 5)\n",
    "    c_p95 = np.nanpercentile(c_vals, 95)\n",
    "    \n",
    "    return (r2_mean, r2_p05, r2_p95), (c_mean, c_p05, c_p95)\n",
    "\n",
    "def geostrophic_vel(field_2d_array):\n",
    "    \"\"\"Calculate geostrophic velocities from 2D SSH field.\"\"\"\n",
    "    dη_dy = np.gradient(field_2d_array, dy, axis=0, edge_order=2)\n",
    "    dη_dx = np.gradient(field_2d_array, dx, axis=1, edge_order=2)\n",
    "    u = -g / f_cor * dη_dy\n",
    "    v =  g / f_cor * dη_dx\n",
    "    return u, v\n",
    "\n",
    "def geostrophic_vel_xarray(da):\n",
    "    \"\"\"Calculate geostrophic velocities from xarray DataArray.\"\"\"\n",
    "    # Apply geostrophic_vel to each sample\n",
    "    u_list = []\n",
    "    v_list = []\n",
    "    for i in range(da.shape[0]):\n",
    "        u, v = geostrophic_vel(da.isel(sample=i).values)\n",
    "        u_list.append(u)\n",
    "        v_list.append(v)\n",
    "    \n",
    "    u_array = np.stack(u_list, axis=0)\n",
    "    v_array = np.stack(v_list, axis=0)\n",
    "    \n",
    "    # Create xarray DataArrays with same structure as input\n",
    "    u_da = da.copy()\n",
    "    u_da.values = u_array\n",
    "    v_da = da.copy() \n",
    "    v_da.values = v_array\n",
    "    \n",
    "    return u_da, v_da\n",
    "\n",
    "# Get true velocities from BM\n",
    "bm_true = test.bm\n",
    "u_true, v_true = geostrophic_vel_xarray(bm_true)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# models to evaluate from eval_datasets\n",
    "# -------------------------------------------------\n",
    "models = {\n",
    "    \"sst_ssh\": eval_datasets['sst_ssh'],\n",
    "    \"ssh_only\": eval_datasets['ssh_only'], \n",
    "    \"mse_only\": eval_datasets['mse_only']\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# main loop\n",
    "# -------------------------------------------------\n",
    "records = []\n",
    "records_p05 = []  # For 5th percentile\n",
    "records_p95 = []  # For 95th percentile\n",
    "\n",
    "for name, dataset in models.items():\n",
    "    print(f\"Processing model: {name}\")\n",
    "    \n",
    "    # Get UBM predictions\n",
    "    ubm_pred = dataset.ubm_pred_mean\n",
    "    \n",
    "    # UBM metrics\n",
    "    (ubm_r2, ubm_r2_p05, ubm_r2_p95), (ubm_corr, ubm_corr_p05, ubm_corr_p95) = r2_corr(\n",
    "        flatten_clean(test.ubm), flatten_clean(ubm_pred))\n",
    "    \n",
    "    # BM metrics - calculate BM from SSH - UBM\n",
    "    bm_pred = test.ssh - ubm_pred\n",
    "    (bm_r2, bm_r2_p05, bm_r2_p95), (bm_corr, bm_corr_p05, bm_corr_p95) = r2_corr(\n",
    "        flatten_clean(bm_true), flatten_clean(bm_pred))\n",
    "    \n",
    "    # velocity metrics\n",
    "    u_pred, v_pred = geostrophic_vel_xarray(bm_pred)\n",
    "    (u_r2, u_r2_p05, u_r2_p95), (u_corr, u_corr_p05, u_corr_p95) = r2_corr(\n",
    "        flatten_clean(u_true), flatten_clean(u_pred))\n",
    "    (v_r2, v_r2_p05, v_r2_p95), (v_corr, v_corr_p05, v_corr_p95) = r2_corr(\n",
    "        flatten_clean(v_true), flatten_clean(v_pred))\n",
    "    \n",
    "    # average of all eight mean numbers\n",
    "    avg_all = np.nanmean([ubm_r2, ubm_corr, bm_r2, bm_corr,\n",
    "                          u_r2, u_corr, v_r2, v_corr])\n",
    "    \n",
    "    # average of all eight p05 numbers\n",
    "    avg_all_p05 = np.nanmean([ubm_r2_p05, ubm_corr_p05, bm_r2_p05, bm_corr_p05,\n",
    "                              u_r2_p05, u_corr_p05, v_r2_p05, v_corr_p05])\n",
    "    \n",
    "    # average of all eight p95 numbers\n",
    "    avg_all_p95 = np.nanmean([ubm_r2_p95, ubm_corr_p95, bm_r2_p95, bm_corr_p95,\n",
    "                              u_r2_p95, u_corr_p95, v_r2_p95, v_corr_p95])\n",
    "    \n",
    "    # Mean metrics\n",
    "    records.append(dict(model=name,\n",
    "                        UBM_R2=ubm_r2, UBM_corr=ubm_corr,\n",
    "                        BM_R2=bm_r2, BM_corr=bm_corr,\n",
    "                        u_R2=u_r2, u_corr=u_corr,\n",
    "                        v_R2=v_r2, v_corr=v_corr,\n",
    "                        avg_all=avg_all))\n",
    "    \n",
    "    # 5th percentile metrics\n",
    "    records_p05.append(dict(model=name,\n",
    "                            UBM_R2=ubm_r2_p05, UBM_corr=ubm_corr_p05,\n",
    "                            BM_R2=bm_r2_p05, BM_corr=bm_corr_p05,\n",
    "                            u_R2=u_r2_p05, u_corr=u_corr_p05,\n",
    "                            v_R2=v_r2_p05, v_corr=v_corr_p05,\n",
    "                            avg_all=avg_all_p05))\n",
    "    \n",
    "    # 95th percentile metrics\n",
    "    records_p95.append(dict(model=name,\n",
    "                            UBM_R2=ubm_r2_p95, UBM_corr=ubm_corr_p95,\n",
    "                            BM_R2=bm_r2_p95, BM_corr=bm_corr_p95,\n",
    "                            u_R2=u_r2_p95, u_corr=u_corr_p95,\n",
    "                            v_R2=v_r2_p95, v_corr=v_corr_p95,\n",
    "                            avg_all=avg_all_p95))\n",
    "\n",
    "# Create DataFrames for mean, p05, and p95\n",
    "metrics_all = (pd.DataFrame(records)\n",
    "               .set_index(\"model\")\n",
    "               .round(3)\n",
    "               .sort_values(\"avg_all\", ascending=False))\n",
    "\n",
    "metrics_p05 = (pd.DataFrame(records_p05)\n",
    "               .set_index(\"model\")\n",
    "               .round(3)\n",
    "               .sort_values(\"avg_all\", ascending=False))\n",
    "\n",
    "metrics_p95 = (pd.DataFrame(records_p95)\n",
    "               .set_index(\"model\")\n",
    "               .round(3)\n",
    "               .sort_values(\"avg_all\", ascending=False))\n",
    "\n",
    "\n",
    "print(\"\\n=== Combined Summary (Mean ± Range) ===\")\n",
    "combined_summary = pd.DataFrame(index=metrics_all.index)\n",
    "\n",
    "for col in ['UBM_R2', 'UBM_corr', 'BM_R2', 'BM_corr', 'u_R2', 'u_corr', 'v_R2', 'v_corr', 'avg_all']:\n",
    "    combined_summary[col] = (metrics_all[col].round(3).astype(str) + \n",
    "                            ' (' + metrics_p05[col].round(3).astype(str) + \n",
    "                            ', ' + metrics_p95[col].round(3).astype(str) + ')')\n",
    "\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366acde2-30a2-486b-8334-b56b9a5c8077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa36db21-75fb-4b87-8747-40c7638e9a94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SI Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadec81-c5c2-45c3-9dfd-2bcfa50d7244",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gaussin Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8b66e97-201e-4010-a937-04f70c1d6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cmocean  # Added import for cmocean\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "best_sample_idx = extreme_samples['sst_ssh']['max']['sample_idx']\n",
    "best_r2_value = extreme_samples['sst_ssh']['max']['r2']\n",
    "\n",
    "# Get the true and predicted BM data for the best sample\n",
    "best_bm_true_sample = eval_datasets['sst_ssh'].bm_truth.isel(sample=best_sample_idx).values\n",
    "best_bm_pred_sample = eval_datasets['sst_ssh'].bm_pred_mean.isel(sample=best_sample_idx).values\n",
    "\n",
    "plt.rcParams.update({\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['Arial', 'DejaVu Sans', 'Helvetica', 'sans-serif'],\n",
    "        'mathtext.fontset': 'stix',  \n",
    "        'axes.grid': False,\n",
    "        'figure.facecolor': 'white',\n",
    "        'axes.facecolor': 'white'\n",
    "    })\n",
    "\n",
    "# Set title parameters\n",
    "title_fontsize = 40 \n",
    "title_pad = 8\n",
    "\n",
    "# Set colorbar tick fontsize\n",
    "colorbar_tick_fontsize = 35\n",
    "\n",
    "# ----------------------\n",
    "# Define plotting parameters\n",
    "# ----------------------\n",
    "# Column 1: Original data\n",
    "vmin_orig, vmax_orig = -0.08, 0.08\n",
    "\n",
    "# Column 2: Features < 60km \n",
    "vmin_60, vmax_60 = -0.02, 0.02\n",
    "\n",
    "# Column 3: Features < 30km \n",
    "vmin_30, vmax_30 = -0.02, 0.02\n",
    "\n",
    "# Column 4: Features < 10km \n",
    "vmin_10, vmax_10 = -0.002, 0.002\n",
    "\n",
    "# Define parameters for normalized error\n",
    "# Using appropriate scales for each column \n",
    "vmin_norm_orig, vmax_norm_orig = 0, 2.0  # Normalized absolute error for original data\n",
    "vmin_norm_60, vmax_norm_60 = 0, 2.0  # Normalized absolute error for 60km features\n",
    "vmin_norm_30, vmax_norm_30 = 0, 2.0  # Normalized absolute error for 30km features\n",
    "vmin_norm_10, vmax_norm_10 = 0, 2.0  # Normalized absolute error for 10km features\n",
    "\n",
    "# Define tick values in actual data units \n",
    "ticks_orig_data = np.array([-0.08, -0.04, 0.0, 0.04, 0.08])\n",
    "ticks_60_data = np.array([-0.02, -0.01, 0.0, 0.01, 0.02])  \n",
    "ticks_30_data = np.array([-0.02, -0.01, 0.0, 0.01, 0.02])  \n",
    "ticks_10_data = np.array([-0.002, -0.001, 0.0, 0.001, 0.002])  \n",
    "\n",
    "# Define ticks for normalized absolute error (dimensionless)\n",
    "ticks_norm_orig_data = np.array([0, 0.5, 1.0, 1.5, 2.0])\n",
    "ticks_norm_60_data = np.array([0, 0.5, 1.0, 1.5, 2.0])\n",
    "ticks_norm_30_data = np.array([0, 0.5, 1.0, 1.5, 2.0])\n",
    "ticks_norm_10_data = np.array([0, 0.5, 1.0, 1.5, 2.0])\n",
    "\n",
    "# Create the size of arrays\n",
    "size = best_bm_true_sample.shape[0]\n",
    "i = np.arange(size)\n",
    "j = np.arange(size)\n",
    "\n",
    "# Calculate Gaussian filter parameters based on physical distance\n",
    "pixel_size = 1.5  # km per grid point\n",
    "\n",
    "# For features smaller than 60km\n",
    "scale_60km = 60 / pixel_size  # Convert to grid units\n",
    "sigma_60km = scale_60km / np.sqrt(12)  # Convert to Gaussian sigma using np.sqrt(12)\n",
    "\n",
    "# For features smaller than 30km\n",
    "scale_30km = 30 / pixel_size  # Convert to grid units\n",
    "sigma_30km = scale_30km / np.sqrt(12)  # Convert to Gaussian sigma using np.sqrt(12)\n",
    "\n",
    "# For features smaller than 10km\n",
    "scale_10km = 10 / pixel_size  # Convert to grid units\n",
    "sigma_10km = scale_10km / np.sqrt(12)  # Convert to Gaussian sigma using np.sqrt(12)\n",
    "\n",
    "# Apply Gaussian filter to get high-pass filtered versions\n",
    "# For 60km features\n",
    "true_low_pass_60km = gaussian_filter(best_bm_true_sample, sigma=sigma_60km)\n",
    "true_high_pass_60km = best_bm_true_sample - true_low_pass_60km\n",
    "pred_low_pass_60km = gaussian_filter(best_bm_pred_sample, sigma=sigma_60km)\n",
    "pred_high_pass_60km = best_bm_pred_sample - pred_low_pass_60km\n",
    "\n",
    "# For 30km features\n",
    "true_low_pass_30km = gaussian_filter(best_bm_true_sample, sigma=sigma_30km)\n",
    "true_high_pass_30km = best_bm_true_sample - true_low_pass_30km\n",
    "pred_low_pass_30km = gaussian_filter(best_bm_pred_sample, sigma=sigma_30km)\n",
    "pred_high_pass_30km = best_bm_pred_sample - pred_low_pass_30km\n",
    "\n",
    "# For 10km features\n",
    "true_low_pass_10km = gaussian_filter(best_bm_true_sample, sigma=sigma_10km)\n",
    "true_high_pass_10km = best_bm_true_sample - true_low_pass_10km\n",
    "pred_low_pass_10km = gaussian_filter(best_bm_pred_sample, sigma=sigma_10km)\n",
    "pred_high_pass_10km = best_bm_pred_sample - pred_low_pass_10km\n",
    "\n",
    "# Crop the arrays to remove boundary effects\n",
    "# Define crop width \n",
    "crop_width = 2\n",
    "\n",
    "# Crop for 60km features\n",
    "true_high_pass_60km_cropped = true_high_pass_60km[crop_width:-crop_width, crop_width:-crop_width]\n",
    "pred_high_pass_60km_cropped = pred_high_pass_60km[crop_width:-crop_width, crop_width:-crop_width]\n",
    "\n",
    "# Crop for 30km features\n",
    "true_high_pass_30km_cropped = true_high_pass_30km[crop_width:-crop_width, crop_width:-crop_width]\n",
    "pred_high_pass_30km_cropped = pred_high_pass_30km[crop_width:-crop_width, crop_width:-crop_width]\n",
    "\n",
    "# Crop for 10km features\n",
    "true_high_pass_10km_cropped = true_high_pass_10km[crop_width:-crop_width, crop_width:-crop_width]\n",
    "pred_high_pass_10km_cropped = pred_high_pass_10km[crop_width:-crop_width, crop_width:-crop_width]\n",
    "\n",
    "# Also crop the original data for consistency\n",
    "best_bm_true_sample_cropped = best_bm_true_sample[crop_width:-crop_width, crop_width:-crop_width]\n",
    "best_bm_pred_sample_cropped = best_bm_pred_sample[crop_width:-crop_width, crop_width:-crop_width]\n",
    "\n",
    "# Calculate normalized absolute errors \n",
    "# Normalized absolute error for original data\n",
    "squared_error_orig = np.abs(best_bm_pred_sample_cropped - best_bm_true_sample_cropped)\n",
    "std_orig = np.std(best_bm_true_sample_cropped)\n",
    "norm_absolute_error_orig = squared_error_orig / std_orig\n",
    "\n",
    "# Normalized absolute error for 60km features\n",
    "squared_error_60km = np.abs(pred_high_pass_60km_cropped - true_high_pass_60km_cropped)\n",
    "std_60km = np.std(true_high_pass_60km_cropped)\n",
    "norm_absolute_error_60km = squared_error_60km / std_60km\n",
    "\n",
    "# Normalized absolute error for 30km features\n",
    "squared_error_30km = np.abs(pred_high_pass_30km_cropped - true_high_pass_30km_cropped)\n",
    "std_30km = np.std(true_high_pass_30km_cropped)\n",
    "norm_absolute_error_30km = squared_error_30km / std_30km\n",
    "\n",
    "# Normalized absolute error for 10km features\n",
    "squared_error_10km = np.abs(pred_high_pass_10km_cropped - true_high_pass_10km_cropped)\n",
    "std_10km = np.std(true_high_pass_10km_cropped)\n",
    "norm_absolute_error_10km = squared_error_10km / std_10km\n",
    "\n",
    "# Create figure with subplots \n",
    "fig = plt.figure(figsize=(38, 20))  \n",
    "\n",
    "# Create a grid with proper alignment - 3 rows, 15 columns\n",
    "gs = gridspec.GridSpec(3, 15,  \n",
    "                      width_ratios=[0.94, 0.001, 0.08, 0.3, 0.94, 0.001, 0.08, 0.3, 0.94, 0.001, 0.08, 0.3, 0.94, 0.001, 0.08],  \n",
    "                      height_ratios=[1, 1, 1],\n",
    "                      wspace=-0.05, hspace=0.12)\n",
    "\n",
    "# Define colormap as cmocean thermal \n",
    "cmap_bm = cmocean.cm.ice\n",
    "cmap_norm_error = cmocean.cm.amp\n",
    "\n",
    "# Helper function to format the axes with centered titles\n",
    "def format_ax(ax, title):\n",
    "    # Remove all ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title(title, fontsize=title_fontsize, pad=title_pad, loc='center')\n",
    "    \n",
    "\n",
    "# Helper function to create colorbar with scientific notation header\n",
    "def create_colorbar_with_scientific(im, cax, ticks, unit, exponent, extend_type='both'):\n",
    "    cbar = plt.colorbar(im, cax=cax, extend=extend_type, ticks=ticks)\n",
    "    \n",
    "    # Set the ylabel with units\n",
    "    cbar.ax.set_ylabel(unit, rotation=270, labelpad=25, fontsize=30)\n",
    "    cbar.ax.yaxis.tick_right()\n",
    "    cbar.ax.yaxis.set_label_position('right')\n",
    "    \n",
    "    # Format tick labels as simple numbers \n",
    "    tick_labels = [f'{x * (10**(-exponent)):.1f}' for x in ticks]\n",
    "    cbar.ax.set_yticklabels(tick_labels)\n",
    "    cbar.ax.tick_params(labelsize=colorbar_tick_fontsize, length=8, width=2)\n",
    "    \n",
    "    # Add scientific notation multiplier at the top center\n",
    "    if exponent != 0:\n",
    "        multiplier_text = f'×10$^{{{exponent}}}$'\n",
    "        cbar.ax.text(0.75, 1.04, multiplier_text, transform=cbar.ax.transAxes, \n",
    "                    ha='center', va='bottom', fontsize=28)\n",
    "    \n",
    "    return cbar\n",
    "\n",
    "# Helper function for normalized error colorbars \n",
    "def create_normalized_colorbar(im, cax, ticks, extend_type='max'):\n",
    "    cbar = plt.colorbar(im, cax=cax, extend=extend_type, ticks=ticks)\n",
    "    cbar.ax.set_ylabel('', rotation=270, labelpad=25, fontsize=30)\n",
    "    cbar.ax.yaxis.tick_right()\n",
    "    cbar.ax.yaxis.set_label_position('right')\n",
    "    cbar.ax.set_yticklabels([f'{x:.1f}' for x in ticks])\n",
    "    cbar.ax.tick_params(labelsize=colorbar_tick_fontsize, length=8, width=2)\n",
    "    return cbar\n",
    "\n",
    "# Row 1: True Data\n",
    "# Column 1: Original data\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "im1 = ax1.imshow(best_bm_true_sample_cropped, cmap=cmap_bm, vmin=vmin_orig, vmax=vmax_orig)\n",
    "format_ax(ax1, '<120 km')\n",
    "\n",
    "# Column 2: Features < 60km \n",
    "ax1_5 = fig.add_subplot(gs[0, 4])\n",
    "im1_5 = ax1_5.imshow(true_high_pass_60km_cropped, cmap=cmap_bm, vmin=vmin_60, vmax=vmax_60)\n",
    "format_ax(ax1_5, '<60 km')\n",
    "\n",
    "# Column 3: Features < 30km\n",
    "ax2 = fig.add_subplot(gs[0, 8])\n",
    "im2 = ax2.imshow(true_high_pass_30km_cropped, cmap=cmap_bm, vmin=vmin_30, vmax=vmax_30)\n",
    "format_ax(ax2, '<30 km')\n",
    "\n",
    "# Column 4: Features < 10km\n",
    "ax3 = fig.add_subplot(gs[0, 12])\n",
    "im3 = ax3.imshow(true_high_pass_10km_cropped, cmap=cmap_bm, vmin=vmin_10, vmax=vmax_10)\n",
    "format_ax(ax3, '<10 km')\n",
    "\n",
    "# Add row label for first row\n",
    "ax1.text(-0.06, 0.5, 'True', transform=ax1.transAxes, rotation=90, \n",
    "         va='center', ha='center', fontsize=title_fontsize)\n",
    "\n",
    "# Row 2: Predicted Data\n",
    "# Column 1: Original data\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "im4 = ax4.imshow(best_bm_pred_sample_cropped, cmap=cmap_bm, vmin=vmin_orig, vmax=vmax_orig)\n",
    "format_ax(ax4, '')\n",
    "\n",
    "# Column 2: Features < 60km\n",
    "ax4_5 = fig.add_subplot(gs[1, 4])\n",
    "im4_5 = ax4_5.imshow(pred_high_pass_60km_cropped, cmap=cmap_bm, vmin=vmin_60, vmax=vmax_60)\n",
    "format_ax(ax4_5, '')\n",
    "\n",
    "# Column 3: Features < 30km\n",
    "ax5 = fig.add_subplot(gs[1, 8])\n",
    "im5 = ax5.imshow(pred_high_pass_30km_cropped, cmap=cmap_bm, vmin=vmin_30, vmax=vmax_30)\n",
    "format_ax(ax5, '')\n",
    "\n",
    "# Column 4: Features < 10km \n",
    "ax6 = fig.add_subplot(gs[1, 12])\n",
    "im6 = ax6.imshow(pred_high_pass_10km_cropped, cmap=cmap_bm, vmin=vmin_10, vmax=vmax_10)\n",
    "format_ax(ax6, '')\n",
    "\n",
    "# Add row label for second row\n",
    "ax4.text(-0.06, 0.5, 'ZCA+SST', transform=ax4.transAxes, rotation=90, \n",
    "         va='center', ha='center', fontsize=title_fontsize)\n",
    "\n",
    "# Row 3: Normalized absolute Error \n",
    "# Column 1: Normalized absolute error for original data\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "im7 = ax7.imshow(norm_absolute_error_orig, cmap=cmap_norm_error, vmin=0, vmax=vmax_norm_orig)\n",
    "format_ax(ax7, '')\n",
    "\n",
    "# Column 2: Normalized absolute error for 60km features \n",
    "ax7_5 = fig.add_subplot(gs[2, 4])\n",
    "im7_5 = ax7_5.imshow(norm_absolute_error_60km, cmap=cmap_norm_error, vmin=0, vmax=vmax_norm_60)\n",
    "format_ax(ax7_5, '')\n",
    "\n",
    "# Column 3: Normalized absolute error for 30km features \n",
    "ax8 = fig.add_subplot(gs[2, 8])\n",
    "im8 = ax8.imshow(norm_absolute_error_30km, cmap=cmap_norm_error, vmin=0, vmax=vmax_norm_30)\n",
    "format_ax(ax8, '')\n",
    "\n",
    "# Column 4: Normalized absolute error for 10km features\n",
    "ax9 = fig.add_subplot(gs[2, 12])\n",
    "im9 = ax9.imshow(norm_absolute_error_10km, cmap=cmap_norm_error, vmin=0, vmax=vmax_norm_10)\n",
    "format_ax(ax9, '')\n",
    "\n",
    "# Add row label for third row\n",
    "ax7.text(-0.06, 0.5, 'Error', transform=ax7.transAxes, rotation=90, \n",
    "         va='center', ha='center', fontsize=title_fontsize)\n",
    "\n",
    "# Create colorbars for each panel with improved scientific notation\n",
    "# Row 1 colorbars (True data)\n",
    "# Colorbar for A1 (original data, -2 exponent)\n",
    "cax1 = fig.add_subplot(gs[0, 2])\n",
    "cbar1 = create_colorbar_with_scientific(im1, cax1, ticks_orig_data, '(m)', -2)\n",
    "\n",
    "# Colorbar for A2 (60km features, -2 exponent)\n",
    "cax1_5 = fig.add_subplot(gs[0, 6])\n",
    "cbar1_5 = create_colorbar_with_scientific(im1_5, cax1_5, ticks_60_data, '(m)', -2)\n",
    "\n",
    "# Colorbar for A3 (30km features, -2 exponent)\n",
    "cax2 = fig.add_subplot(gs[0, 10])\n",
    "cbar2 = create_colorbar_with_scientific(im2, cax2, ticks_30_data, '(m)', -2)\n",
    "\n",
    "# Colorbar for A4 (10km features, -3 exponent)\n",
    "cax3 = fig.add_subplot(gs[0, 14])\n",
    "cbar3 = create_colorbar_with_scientific(im3, cax3, ticks_10_data, '(m)', -3)\n",
    "\n",
    "# Row 2 colorbars (Predicted data)\n",
    "# Colorbar for B1\n",
    "cax4 = fig.add_subplot(gs[1, 2])\n",
    "cbar4 = create_colorbar_with_scientific(im4, cax4, ticks_orig_data, '(m)', -2)\n",
    "\n",
    "# Colorbar for B2\n",
    "cax4_5 = fig.add_subplot(gs[1, 6])\n",
    "cbar4_5 = create_colorbar_with_scientific(im4_5, cax4_5, ticks_60_data, '(m)', -2)\n",
    "\n",
    "# Colorbar for B3\n",
    "cax5 = fig.add_subplot(gs[1, 10])\n",
    "cbar5 = create_colorbar_with_scientific(im5, cax5, ticks_30_data, '(m)', -2)\n",
    "\n",
    "# Colorbar for B4\n",
    "cax6 = fig.add_subplot(gs[1, 14])\n",
    "cbar6 = create_colorbar_with_scientific(im6, cax6, ticks_10_data, '(m)', -3)\n",
    "\n",
    "# Row 3 colorbars (Normalized Absolute Error)\n",
    "# Colorbar for C1\n",
    "cax7 = fig.add_subplot(gs[2, 2])\n",
    "cbar7 = create_normalized_colorbar(im7, cax7, ticks_norm_orig_data)\n",
    "\n",
    "# Colorbar for C2\n",
    "cax7_5 = fig.add_subplot(gs[2, 6])\n",
    "cbar7_5 = create_normalized_colorbar(im7_5, cax7_5, ticks_norm_60_data)\n",
    "\n",
    "# Colorbar for C3\n",
    "cax8 = fig.add_subplot(gs[2, 10])\n",
    "cbar8 = create_normalized_colorbar(im8, cax8, ticks_norm_30_data)\n",
    "\n",
    "# Colorbar for C4\n",
    "cax9 = fig.add_subplot(gs[2, 14])\n",
    "cbar9 = create_normalized_colorbar(im9, cax9, ticks_norm_10_data)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('/home/jovyan/GRL_ssh/figures/SI/scales_bm.png', bbox_inches='tight', dpi=300, transparent=True)\n",
    "plt.savefig('/home/jovyan/GRL_ssh/figures/SI/scales_bm.pdf', bbox_inches='tight', dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef6f59-0444-47dc-bf8b-82fea3b12ae5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e7125d-3edf-4c62-b500-6dd9ab010cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 18,\n",
    "    'font.family': 'serif',\n",
    "    'axes.linewidth': 1.2,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'xtick.major.width': 1.2,\n",
    "    'ytick.major.width': 1.2,\n",
    "    'xtick.minor.width': 0.8,\n",
    "    'ytick.minor.width': 0.8,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.linewidth': 0.8,\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "    'axes.labelsize': 18,\n",
    "    'axes.titlesize': 18\n",
    "})\n",
    "\n",
    "\n",
    "models = ['mse_only', 'ssh_only', 'sst_ssh']\n",
    "model_titles = ['MSE+SST', 'ZCA', 'ZCA+SST']\n",
    "\n",
    "# Define outlier thresholds\n",
    "R2_MIN = 0.001  # Minimum R^2 to include (removes very negative values)\n",
    "R2_MAX = 1.0\n",
    "REL_MAG_MIN = 0.001  # Minimum relative magnitude\n",
    "REL_MAG_MAX = 1   # Maximum relative magnitude\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(22, 12))\n",
    "\n",
    "\n",
    "for col, model in enumerate(models):\n",
    "\n",
    "    ubm_truth = eval_datasets[model].ubm_truth\n",
    "    bm_truth = eval_datasets[model].bm_truth\n",
    "    ubm_pred = eval_datasets[model].ubm_pred_mean\n",
    "    bm_pred = eval_datasets[model].bm_pred_mean\n",
    "    \n",
    "    # Get number of samples\n",
    "    n_samples = ubm_truth.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store metrics for each sample\n",
    "    bm_r2_values = []\n",
    "    ubm_r2_values = []\n",
    "    relative_magnitudes = []\n",
    "    \n",
    "    # Calculate metrics for each sample\n",
    "    for sample_idx in range(n_samples):\n",
    "        # Get 2D fields for current sample\n",
    "        ubm_true_2d = ubm_truth[sample_idx, :, :].values.flatten()\n",
    "        bm_true_2d = bm_truth[sample_idx, :, :].values.flatten()\n",
    "        ubm_pred_2d = ubm_pred[sample_idx, :, :].values.flatten()\n",
    "        bm_pred_2d = bm_pred[sample_idx, :, :].values.flatten()\n",
    "        \n",
    "        # Remove NaN values if any\n",
    "        valid_mask_ubm = ~(np.isnan(ubm_true_2d) | np.isnan(ubm_pred_2d))\n",
    "        valid_mask_bm = ~(np.isnan(bm_true_2d) | np.isnan(bm_pred_2d))\n",
    "        \n",
    "        # Calculate R^2 for UBM and BM\n",
    "        if np.sum(valid_mask_ubm) > 1:\n",
    "            ubm_r2 = r2_score(ubm_true_2d[valid_mask_ubm], ubm_pred_2d[valid_mask_ubm])\n",
    "        else:\n",
    "            ubm_r2 = np.nan\n",
    "            \n",
    "        if np.sum(valid_mask_bm) > 1:\n",
    "            bm_r2 = r2_score(bm_true_2d[valid_mask_bm], bm_pred_2d[valid_mask_bm])\n",
    "        else:\n",
    "            bm_r2 = np.nan\n",
    "        \n",
    "        # Calculate relative magnitude (mean absolute values to avoid division issues)\n",
    "        # Check if we have valid data before calculating magnitudes\n",
    "        if np.sum(~np.isnan(ubm_true_2d)) > 0:\n",
    "            ubm_magnitude = np.nanmean(np.abs(ubm_true_2d))\n",
    "        else:\n",
    "            ubm_magnitude = np.nan\n",
    "            \n",
    "        if np.sum(~np.isnan(bm_true_2d)) > 0:\n",
    "            bm_magnitude = np.nanmean(np.abs(bm_true_2d))\n",
    "        else:\n",
    "            bm_magnitude = np.nan\n",
    "        \n",
    "        if not np.isnan(bm_magnitude) and bm_magnitude != 0 and not np.isnan(ubm_magnitude):\n",
    "            rel_magnitude = ubm_magnitude / bm_magnitude\n",
    "        else:\n",
    "            rel_magnitude = np.nan\n",
    "        \n",
    "        # Store values\n",
    "        bm_r2_values.append(bm_r2)\n",
    "        ubm_r2_values.append(ubm_r2)\n",
    "        relative_magnitudes.append(rel_magnitude)\n",
    "    \n",
    "    # Convert to numpy arrays and remove NaN values for plotting\n",
    "    bm_r2_values = np.array(bm_r2_values)\n",
    "    ubm_r2_values = np.array(ubm_r2_values)\n",
    "    relative_magnitudes = np.array(relative_magnitudes)\n",
    "    \n",
    "    # Plot BM (top row)\n",
    "    valid_bm = ~(np.isnan(bm_r2_values) | np.isnan(relative_magnitudes))\n",
    "    if np.sum(valid_bm) > 0:\n",
    "        # Apply outlier filtering\n",
    "        bm_r2_filt = bm_r2_values[valid_bm]\n",
    "        rel_mag_filt = relative_magnitudes[valid_bm]\n",
    "        \n",
    "        # Filter outliers\n",
    "        outlier_mask = ((bm_r2_filt >= R2_MIN) & (bm_r2_filt <= R2_MAX) & \n",
    "                       (rel_mag_filt >= REL_MAG_MIN) & (rel_mag_filt <= REL_MAG_MAX))\n",
    "        \n",
    "        if np.sum(outlier_mask) > 0:\n",
    "            hb = axes[0, col].hexbin(rel_mag_filt[outlier_mask], bm_r2_filt[outlier_mask],\n",
    "                                   gridsize=60, cmap='Reds', mincnt=1, \n",
    "                                   xscale='log', yscale='log', vmin=0, vmax=3)\n",
    "    \n",
    "    axes[0, col].set_title(f'{model_titles[col]} (BM)', fontsize=18, fontweight='bold', pad=20)\n",
    "    # Only add x-axis label for bottom row\n",
    "    if col == 0:  # Only label y-axis for leftmost column\n",
    "        axes[0, col].set_ylabel('R²', fontsize=16)\n",
    "    axes[0, col].set_xscale('log')\n",
    "    axes[0, col].set_yscale('log')\n",
    "    axes[0, col].set_xlim(1e-3, 1e0)\n",
    "    axes[0, col].set_ylim(1e-3, 1e0)\n",
    "    axes[0, col].tick_params(which='both', direction='in', top=True, right=True)\n",
    "    \n",
    "    # Plot UBM (bottom row)\n",
    "    valid_ubm = ~(np.isnan(ubm_r2_values) | np.isnan(relative_magnitudes))\n",
    "    if np.sum(valid_ubm) > 0:\n",
    "        # Apply outlier filtering\n",
    "        ubm_r2_filt = ubm_r2_values[valid_ubm]\n",
    "        rel_mag_filt = relative_magnitudes[valid_ubm]\n",
    "        \n",
    "        # Filter outliers\n",
    "        outlier_mask = ((ubm_r2_filt >= R2_MIN) & (ubm_r2_filt <= R2_MAX) & \n",
    "                       (rel_mag_filt >= REL_MAG_MIN) & (rel_mag_filt <= REL_MAG_MAX))\n",
    "        \n",
    "        if np.sum(outlier_mask) > 0:\n",
    "            hb = axes[1, col].hexbin(rel_mag_filt[outlier_mask], ubm_r2_filt[outlier_mask],\n",
    "                                   gridsize=60, cmap='Blues', mincnt=1, \n",
    "                                   xscale='log', yscale='log', vmin=0, vmax=3)\n",
    "    \n",
    "    axes[1, col].set_title(f'{model_titles[col]} (UBM)', fontsize=18, fontweight='bold', pad=20)\n",
    "    axes[1, col].set_xlabel('Relative Magnitude (UBM/BM)', fontsize=16)  # x-axis label for bottom row\n",
    "    if col == 0:  # Only label y-axis for leftmost column\n",
    "        axes[1, col].set_ylabel('R²', fontsize=16)\n",
    "    axes[1, col].set_xscale('log')\n",
    "    axes[1, col].set_yscale('log')\n",
    "    axes[1, col].set_xlim(1e-3, 1e0)\n",
    "    axes[1, col].set_ylim(1e-3, 1e0)\n",
    "    axes[1, col].tick_params(which='both', direction='in', top=True, right=True)\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93]) \n",
    "plt.subplots_adjust(hspace=0.35, wspace=0.15)  \n",
    "\n",
    "plt.savefig('figures/SI/scatter.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.savefig('figures/SI/scatter.pdf', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c294bf7-0d75-4bfe-aedd-cb5f05509c27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## R^2 & Std Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94f327ab-1b28-4938-980e-075acb94a96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved to: figures/SI/performance_maps.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.metrics import r2_score\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "def calculate_spatial_r2(true_data, pred_data):\n",
    "    \"\"\"\n",
    "    Calculate R^2 for each spatial location across the time dimension.\n",
    "    \n",
    "    Args:\n",
    "        true_data: Array of shape (n_samples, H, W)\n",
    "        pred_data: Array of shape (n_samples, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        r2_map: Array of shape (H, W) with R^2 values for each spatial location\n",
    "    \"\"\"\n",
    "    n_samples, H, W = true_data.shape\n",
    "    r2_map = np.zeros((H, W))\n",
    "    \n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            # Extract time series for this spatial location\n",
    "            true_ts = true_data[:, i, j]\n",
    "            pred_ts = pred_data[:, i, j]\n",
    "            \n",
    "            # Filter out NaN values\n",
    "            valid_mask = ~(np.isnan(true_ts) | np.isnan(pred_ts))\n",
    "            true_ts_valid = true_ts[valid_mask]\n",
    "            pred_ts_valid = pred_ts[valid_mask]\n",
    "            \n",
    "            if len(true_ts_valid) < 2 or np.var(true_ts_valid) == 0:\n",
    "                r2_map[i, j] = np.nan\n",
    "            else:\n",
    "                r2_map[i, j] = r2_score(true_ts_valid, pred_ts_valid)\n",
    "    \n",
    "    return r2_map\n",
    "\n",
    "def create_discrete_colormap_and_norm(data, cmap_name, n_levels=10, percentile_range=(5, 95)):\n",
    "    \"\"\"\n",
    "    Create discrete colormap and normalization based on data distribution.\n",
    "    \n",
    "    Args:\n",
    "        data: Input data array\n",
    "        cmap_name: Name of the colormap\n",
    "        n_levels: Number of discrete levels\n",
    "        percentile_range: Tuple of (low, high) percentiles for range\n",
    "    \n",
    "    Returns:\n",
    "        cmap, norm, levels\n",
    "    \"\"\"\n",
    "    # Get data range based on percentiles\n",
    "    vmin, vmax = np.nanpercentile(data, percentile_range)\n",
    "    \n",
    "    # Create discrete levels\n",
    "    levels = np.linspace(vmin, vmax, n_levels + 1)\n",
    "    \n",
    "    # Create discrete colormap and normalization\n",
    "    cmap = plt.cm.get_cmap(cmap_name, n_levels)\n",
    "    norm = BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
    "    \n",
    "    return cmap, norm, levels\n",
    "\n",
    "def plot_r2_and_uncertainty_maps(eval_dataset, figsize=(12, 6), dpi=300, save_path=\"figures/\", \n",
    "                                 n_levels_r2=10, n_levels_unc=10):\n",
    "\n",
    "    plt.style.use('default')  \n",
    "    \n",
    "    plt.rcParams.update({\n",
    "        'font.size': 11,\n",
    "        'font.family': 'sans-serif',\n",
    "        'axes.linewidth': 1.2,\n",
    "        'axes.spines.top': True,\n",
    "        'axes.spines.right': True,\n",
    "        'axes.spines.bottom': True,\n",
    "        'axes.spines.left': True,\n",
    "        'axes.grid': False,\n",
    "        'xtick.direction': 'in',\n",
    "        'ytick.direction': 'in',\n",
    "        'xtick.major.size': 4,\n",
    "        'ytick.major.size': 4,\n",
    "        'legend.frameon': True,\n",
    "        'legend.fancybox': False,\n",
    "        'legend.shadow': False,\n",
    "        'legend.edgecolor': 'black',\n",
    "        'savefig.dpi': dpi,\n",
    "        'savefig.bbox': 'tight',\n",
    "        'savefig.pad_inches': 0.1\n",
    "    })\n",
    "    \n",
    "\n",
    "    # Extract the data \n",
    "    ubm_true = eval_dataset['ubm_truth'].values\n",
    "    ubm_pred = eval_dataset['ubm_pred_mean'].values\n",
    "    \n",
    "    # Calculate R^2 map\n",
    "    ubm_r2_map = calculate_spatial_r2(ubm_true, ubm_pred)\n",
    "    ubm_r2_map = np.clip(ubm_r2_map, -1.0, 1.0)\n",
    "    \n",
    "    # Uncertainty calculation\n",
    "    # Extract ensemble predictions\n",
    "    ubm_ensemble = eval_dataset['ubm_pred_samples'].values\n",
    "\n",
    "    # Calculate std for each sample\n",
    "    ubm_ensemble_std_per_sample = np.std(ubm_ensemble, axis=1)  # Shape: (n_test_samples, H, W)\n",
    "    \n",
    "    # Geometric mean via log-space averaging\n",
    "    ubm_uncertainty = np.exp(np.mean(np.log(ubm_ensemble_std_per_sample + 1e-8), axis=0))  # Shape: (H, W)\n",
    "    \n",
    "    # Create discrete colormaps and normalizations\n",
    "    r2_cmap, r2_norm, r2_levels = create_discrete_colormap_and_norm(\n",
    "        ubm_r2_map, 'RdYlBu_r', n_levels_r2, (5, 95)\n",
    "    )\n",
    "    \n",
    "    unc_cmap, unc_norm, unc_levels = create_discrete_colormap_and_norm(\n",
    "        ubm_uncertainty, 'plasma', n_levels_unc, (5, 95)\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize, facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Set up distance ticks\n",
    "    size = 80\n",
    "    tick_positions = np.linspace(0, size-1, 9)\n",
    "    tick_positions = np.round(tick_positions).astype(int)\n",
    "    km_ticks = tick_positions * 1.5  \n",
    "    \n",
    "    x_labels = []\n",
    "    y_labels = []\n",
    "    for i, km in enumerate(km_ticks):\n",
    "        if i % 2 == 0: \n",
    "            x_labels.append(f'{km:.0f}')\n",
    "            y_labels.append(f'{km:.0f}')\n",
    "        else:\n",
    "            x_labels.append('')\n",
    "            y_labels.append('')\n",
    "    \n",
    "    # First column\n",
    "    im1 = axes[0].imshow(ubm_r2_map, cmap=r2_cmap, norm=r2_norm, \n",
    "                          origin='lower', interpolation='bilinear')\n",
    "    \n",
    "    # Format axis with distance ticks \n",
    "    axes[0].set_xticks(tick_positions)\n",
    "    axes[0].set_yticks(tick_positions)\n",
    "    axes[0].set_xticklabels(x_labels)\n",
    "    axes[0].set_yticklabels(y_labels)\n",
    "    axes[0].tick_params(axis='both', which='major', length=7, width=1.5, labelsize=14)\n",
    "    axes[0].set_xlabel('Distance (km)', fontsize=18)\n",
    "    axes[0].set_ylabel('Distance (km)', fontsize=18)\n",
    "    axes[0].text(0.5, 1.05, '(a)', transform=axes[0].transAxes, \n",
    "             fontsize=26, fontweight='bold', ha='center', va='bottom')\n",
    "    \n",
    "    for spine in axes[0].spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "        spine.set_linewidth(1.2)\n",
    "    axes[0].grid(False)\n",
    "    \n",
    "    # Add discrete colorbar \n",
    "    cbar1 = fig.colorbar(im1, ax=axes[0], shrink=0.62, aspect=20, extend='both', \n",
    "                         boundaries=r2_levels, ticks=r2_levels)\n",
    "    cbar1.set_label(r'$R^2$', rotation=270, labelpad=25, fontsize=20)\n",
    "    cbar1.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # Format colorbar ticks\n",
    "    if np.max(np.abs(r2_levels)) < 1e-2:  # Use scientific notation\n",
    "        cbar1.ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.2e}'))\n",
    "    else:\n",
    "        cbar1.ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.2f}'))\n",
    "    \n",
    "    # Second column: Uncertainty plot with discrete colorbar\n",
    "    im2 = axes[1].imshow(ubm_uncertainty, cmap=unc_cmap, norm=unc_norm, \n",
    "                          origin='lower', interpolation='bilinear')\n",
    "    \n",
    "    # Format axis with distance ticks\n",
    "    axes[1].set_xticks(tick_positions)\n",
    "    axes[1].set_yticks(tick_positions)\n",
    "    axes[1].set_xticklabels(x_labels)\n",
    "    axes[1].set_yticklabels(y_labels)\n",
    "    axes[1].tick_params(axis='both', which='major', length=7, width=1.5, labelsize=14)\n",
    "    axes[1].set_xlabel('Distance (km)', fontsize=18)\n",
    "    axes[1].set_ylabel('Distance (km)', fontsize=18)\n",
    "    axes[1].text(0.5, 1.05, '(b)', transform=axes[1].transAxes,\n",
    "             fontsize=26, fontweight='bold', ha='center', va='bottom')\n",
    "    \n",
    "    for spine in axes[1].spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "        spine.set_linewidth(1.2)\n",
    "    axes[1].grid(False)\n",
    "    \n",
    "    # Add discrete colorbar for uncertainty\n",
    "    cbar2 = fig.colorbar(im2, ax=axes[1], shrink=0.62, aspect=20, extend='both',\n",
    "                         boundaries=unc_levels, ticks=unc_levels)\n",
    "    cbar2.set_label(r'$\\sigma_{geo}$', rotation=270, labelpad=30, fontsize=22)\n",
    "    cbar2.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # Format uncertainty colorbar ticks\n",
    "    cbar2.ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.2e}'))\n",
    "    \n",
    "    # Adjust layout with proper spacing\n",
    "    plt.tight_layout(pad=2.0, w_pad=3.0, h_pad=3.0)\n",
    "    \n",
    "    # Save figure\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        full_path = os.path.join(save_path, 'performance_maps.png')\n",
    "        plt.savefig(full_path, dpi=dpi, bbox_inches='tight', \n",
    "                   facecolor='white', edgecolor='none')\n",
    "        print(f\"Figure saved to: {full_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig, ubm_r2_map, ubm_uncertainty\n",
    "\n",
    "\n",
    "fig, ubm_r2_map, ubm_uncertainty = plot_r2_and_uncertainty_maps(\n",
    "    eval_datasets['sst_ssh'], \n",
    "    figsize=(12, 6), \n",
    "    dpi=300,\n",
    "    save_path='figures/SI',\n",
    "    n_levels_r2=10,      \n",
    "    n_levels_unc=10    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4ea72-8abe-425e-ba69-3526db21f702",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3 PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "977ad975-86f2-489a-a67d-27ff44ef954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSDs for best sample #3041...\n",
      "Computing PSDs for median sample #519...\n",
      "Computing PSDs for worst sample #3323...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import xarray as xr\n",
    "import xrft\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='xrft')\n",
    "\n",
    "def get_sample_psd_data(sample_idx, eval_data):\n",
    "    \"\"\"Get PSD data for a specific sample using ensemble mean PSDs\"\"\"\n",
    "    # Get data for the sample\n",
    "    ubm_true = eval_data.ubm_truth.isel(sample=sample_idx).values\n",
    "    bm_true = eval_data.bm_truth.isel(sample=sample_idx).values\n",
    "    \n",
    "    # Get ensemble data\n",
    "    ubm_ensemble = eval_data.ubm_pred_samples.isel(sample=sample_idx).values\n",
    "    bm_ensemble = eval_data.bm_pred_samples.isel(sample=sample_idx).values\n",
    "    \n",
    "    # Calculate PSDs for truth\n",
    "    psd_ubm_true = calculate_psd_km(ubm_true)\n",
    "    psd_bm_true = calculate_psd_km(bm_true)\n",
    "    \n",
    "    # Calculate ensemble PSDs\n",
    "    ubm_ensemble_psds = []\n",
    "    bm_ensemble_psds = []\n",
    "    \n",
    "    for ens in range(30):\n",
    "        if not np.all(np.isnan(ubm_ensemble[ens, :, :])):\n",
    "            ubm_psd = calculate_psd_km(ubm_ensemble[ens, :, :])\n",
    "            ubm_ensemble_psds.append(ubm_psd.values)\n",
    "            \n",
    "        if not np.all(np.isnan(bm_ensemble[ens, :, :])):\n",
    "            bm_psd = calculate_psd_km(bm_ensemble[ens, :, :])\n",
    "            bm_ensemble_psds.append(bm_psd.values)\n",
    "    \n",
    "    # Calculate envelope bounds (no mean needed)\n",
    "    ubm_ensemble_psds = np.array(ubm_ensemble_psds)\n",
    "    bm_ensemble_psds = np.array(bm_ensemble_psds)\n",
    "    \n",
    "    ubm_psd_05 = np.nanpercentile(ubm_ensemble_psds, 5, axis=0)\n",
    "    ubm_psd_95 = np.nanpercentile(ubm_ensemble_psds, 95, axis=0)\n",
    "    bm_psd_05 = np.nanpercentile(bm_ensemble_psds, 5, axis=0)\n",
    "    bm_psd_95 = np.nanpercentile(bm_ensemble_psds, 95, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'freq': psd_ubm_true.freq_r.values,\n",
    "        'ubm_true': psd_ubm_true.values,\n",
    "        'bm_true': psd_bm_true.values,\n",
    "        'ubm_05': ubm_psd_05,\n",
    "        'ubm_95': ubm_psd_95,\n",
    "        'bm_05': bm_psd_05,\n",
    "        'bm_95': bm_psd_95\n",
    "    }\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'DejaVu Sans', 'Helvetica', 'sans-serif'],\n",
    "    'mathtext.fontset': 'stix',  \n",
    "    'axes.grid': False,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white', \n",
    "    'text.usetex': False,\n",
    "})\n",
    "\n",
    "\n",
    "sample_types = ['best', 'median', 'worst']\n",
    "sample_keys = ['max', 'median', 'min']\n",
    "sample_indices = []\n",
    "\n",
    "for sample_key in sample_keys:\n",
    "    sample_idx = extreme_samples['sst_ssh'][sample_key]['sample_idx']\n",
    "    sample_indices.append(sample_idx)\n",
    "\n",
    "\n",
    "# Load the ZCA+SST evaluation dataset\n",
    "sst_ssh_data = eval_datasets['sst_ssh']\n",
    "\n",
    "# Create figure with 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "colors = {\n",
    "    'ubm_true': '#d62728',      # Red\n",
    "    'bm_true': '#1f77b4',       # Blue\n",
    "    'ubm_envelope': '#ff7f0e',  # Orange\n",
    "    'bm_envelope': '#2ca02c',   # Green\n",
    "}\n",
    "\n",
    "# Per-panel R^2 for UBM prediction (best, median, worst)\n",
    "r2_vals = [0.9127, 0.0866, -6.2663]\n",
    "\n",
    "# Loop through each sample and create plots\n",
    "for i, (sample_idx, sample_type) in enumerate(zip(sample_indices, sample_types)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get PSD data for this sample\n",
    "    print(f\"Computing PSDs for {sample_type} sample #{sample_idx}...\")\n",
    "    psd_data = get_sample_psd_data(sample_idx, sst_ssh_data)\n",
    "    \n",
    "    # Plot true values (solid lines)\n",
    "    ax.loglog(psd_data['freq'], psd_data['ubm_true'], \n",
    "             color=colors['ubm_true'], linewidth=3.5, label='UBM True')\n",
    "    ax.loglog(psd_data['freq'], psd_data['bm_true'], \n",
    "             color=colors['bm_true'], linewidth=3.5, label='BM True')\n",
    "    \n",
    "    # Add uncertainty envelopes (no mean lines)\n",
    "    ax.fill_between(psd_data['freq'], psd_data['ubm_05'], psd_data['ubm_95'], \n",
    "                    alpha=0.6, color=colors['ubm_envelope'], label='UBM 90% CI', \n",
    "                    edgecolor='none')\n",
    "    ax.fill_between(psd_data['freq'], psd_data['bm_05'], psd_data['bm_95'], \n",
    "                    alpha=0.6, color=colors['bm_envelope'], label='BM 90% CI', \n",
    "                    edgecolor='none')\n",
    "    \n",
    "    # Customize plot appearance\n",
    "    ax.set_xlim(8e-3, 5e-1)\n",
    "    ax.set_ylim(1e-12, 1e1)\n",
    "    \n",
    "    # Set custom tick locations for y-axis\n",
    "    y_ticks_labeled = [10**j for j in range(-10, 1, 2)]\n",
    "    ax.yaxis.set_major_locator(ticker.FixedLocator(y_ticks_labeled))\n",
    "    ax.yaxis.set_major_formatter(ticker.LogFormatterMathtext())\n",
    "    \n",
    "    # Professional tick styling\n",
    "    ax.tick_params(axis='both', which='major', labelsize=22, length=8, \n",
    "                   width=1.5, direction='in') \n",
    "    ax.tick_params(axis='both', which='minor', length=4, \n",
    "                   width=0.8, direction='in')\n",
    "\n",
    "    # Hide y-axis labels for middle and right subplots\n",
    "    if i > 0:\n",
    "        ax.tick_params(axis='y', labelleft=False)\n",
    "\n",
    "    # Panel labels\n",
    "    letters = ['a', 'b', 'c']\n",
    "    ax.text(0.02, 0.98, f'{letters[i]})', transform=ax.transAxes, \n",
    "            fontsize=24, fontweight='bold', va='top', ha='left')\n",
    "    \n",
    "    # X-axis label for all subplots\n",
    "    ax.set_xlabel(r'Wavenumber (cpkm)', fontsize=24, fontweight='normal')\n",
    "\n",
    "    # Y-label only on leftmost subplot\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'PSD (m$^2$ cpkm$^{-1}$)', fontsize=24, fontweight='normal')\n",
    "    \n",
    "    # Annotation inside each panel (top-right corner)\n",
    "    ax.text(\n",
    "        0.98, 0.95, f\"R$^2$ (UBM pred) = {r2_vals[i]:.4f}\",\n",
    "        transform=ax.transAxes, ha='right', va='top',\n",
    "        fontsize=18\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles, labels,\n",
    "    loc='lower center', bbox_to_anchor=(0.5, -0.15),\n",
    "    ncol=4, frameon=False, fontsize=22, columnspacing=2.5\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2) \n",
    "plt.show()\n",
    "\n",
    "plt.savefig('/home/jovyan/GRL_ssh/figures/SI/3_psd.png', \n",
    "            bbox_inches='tight', dpi=300, facecolor='white', \n",
    "            edgecolor='none', format='png')\n",
    "plt.savefig('/home/jovyan/GRL_ssh/figures/SI/3_psd.pdf', \n",
    "            bbox_inches='tight', facecolor='white', \n",
    "            edgecolor='none', format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eab1a7-a1a1-4904-be44-61cfe8dcd96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
